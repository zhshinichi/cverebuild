#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¢å¼º webdata.json ä¸­çš„ CVE æ•°æ®
- ä» GitHub API è·å– patch commit å†…å®¹
- ä» GitHub Security Advisories è·å– sec_adv å†…å®¹
- ç”Ÿæˆ sw_version_wget é“¾æ¥
"""

import json
import os
import re
import time
import requests
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# è·¯å¾„é…ç½®
BASE_DIR = Path(__file__).parent.parent
WEBDATA_FILE = BASE_DIR / "large_scale" / "webdata.json"
OUTPUT_FILE = BASE_DIR / "large_scale" / "webdata_enriched.json"
CVELIST_2025 = BASE_DIR / "cvelist" / "2025"

# GitHub Token (ä»ç¯å¢ƒå˜é‡è¯»å–)
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', '')

def get_github_headers():
    """è·å– GitHub API headers"""
    headers = {
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "CVE-Enricher"
    }
    if GITHUB_TOKEN:
        headers["Authorization"] = f"token {GITHUB_TOKEN}"
    return headers


def extract_github_info_from_url(url: str) -> Optional[Dict]:
    """ä» GitHub URL æå– owner/repo/commit ä¿¡æ¯"""
    # åŒ¹é… commit URL
    commit_pattern = r'github\.com/([^/]+)/([^/]+)/commit/([a-f0-9]+)'
    match = re.search(commit_pattern, url)
    if match:
        return {
            'owner': match.group(1),
            'repo': match.group(2),
            'commit': match.group(3),
            'type': 'commit'
        }
    
    # åŒ¹é… security advisory URL
    advisory_pattern = r'github\.com/([^/]+)/([^/]+)/security/advisories/(GHSA-[a-z0-9-]+)'
    match = re.search(advisory_pattern, url)
    if match:
        return {
            'owner': match.group(1),
            'repo': match.group(2),
            'ghsa_id': match.group(3),
            'type': 'advisory'
        }
    
    # åŒ¹é… release URL
    release_pattern = r'github\.com/([^/]+)/([^/]+)/releases/tag/([^/]+)'
    match = re.search(release_pattern, url)
    if match:
        return {
            'owner': match.group(1),
            'repo': match.group(2),
            'tag': match.group(3),
            'type': 'release'
        }
    
    return None


def fetch_commit_content(owner: str, repo: str, commit_hash: str) -> str:
    """ä» GitHub API è·å– commit å†…å®¹"""
    url = f"https://api.github.com/repos/{owner}/{repo}/commits/{commit_hash}"
    
    try:
        response = requests.get(url, headers=get_github_headers(), timeout=30)
        
        if response.status_code == 403:
            print(f"    âš ï¸ Rate limit - waiting 60s...")
            time.sleep(60)
            response = requests.get(url, headers=get_github_headers(), timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            
            # æ„å»º commit å†…å®¹
            content_parts = [data['commit']['message']]
            
            for file in data.get('files', [])[:10]:  # é™åˆ¶æ–‡ä»¶æ•°é‡
                file_content = f"\nFilename: {file['filename']}:\n"
                if 'patch' in file:
                    # åªå–å‰ 2000 å­—ç¬¦
                    patch = file['patch'][:2000]
                    file_content += f"```\n{patch}\n```"
                content_parts.append(file_content)
            
            return '\n'.join(content_parts)
        else:
            print(f"    âš ï¸ Failed to fetch commit: HTTP {response.status_code}")
            return ""
    except Exception as e:
        print(f"    âš ï¸ Error fetching commit: {e}")
        return ""


def fetch_advisory_content(url: str) -> str:
    """è·å– security advisory å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨ requestsï¼‰"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            # ç®€å•æå–æ–‡æœ¬å†…å®¹
            from html.parser import HTMLParser
            
            class TextExtractor(HTMLParser):
                def __init__(self):
                    super().__init__()
                    self.text = []
                    self.in_body = False
                    
                def handle_starttag(self, tag, attrs):
                    if tag == 'body':
                        self.in_body = True
                        
                def handle_data(self, data):
                    if self.in_body:
                        text = data.strip()
                        if text:
                            self.text.append(text)
            
            parser = TextExtractor()
            parser.feed(response.text)
            
            # è¿”å›å‰ 5000 å­—ç¬¦
            return ' '.join(parser.text)[:5000]
        else:
            return ""
    except Exception as e:
        print(f"    âš ï¸ Error fetching advisory: {e}")
        return ""


def get_cve_raw_data(cve_id: str) -> Optional[Dict]:
    """è·å– CVE åŸå§‹æ•°æ®"""
    parts = cve_id.split("-")
    if len(parts) != 3:
        return None
    
    num = parts[2]
    if len(num) <= 4:
        subdir = f"{num[0]}xxx"
    else:
        subdir = f"{num[:2]}xxx"
    
    cve_file = CVELIST_2025 / subdir / f"{cve_id}.json"
    
    if cve_file.exists():
        with open(cve_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def extract_all_urls(cve_raw: Dict) -> List[str]:
    """ä»åŸå§‹ CVE æ•°æ®æå–æ‰€æœ‰ URL"""
    urls = []
    try:
        cna = cve_raw.get("containers", {}).get("cna", {})
        for ref in cna.get("references", []):
            url = ref.get("url")
            if url:
                urls.append(url)
        
        # ä¹Ÿæ£€æŸ¥ adp å®¹å™¨
        for adp in cve_raw.get("containers", {}).get("adp", []):
            for ref in adp.get("references", []):
                url = ref.get("url")
                if url:
                    urls.append(url)
    except:
        pass
    return urls


def generate_wget_url(owner: str, repo: str, version: str) -> str:
    """ç”Ÿæˆä¸‹è½½ URL"""
    if not version:
        return ""
    
    # æ¸…ç†ç‰ˆæœ¬å·
    clean_version = version.strip()
    if clean_version.startswith(">=") or clean_version.startswith("<="):
        clean_version = clean_version[2:].strip()
    elif clean_version.startswith(">") or clean_version.startswith("<"):
        clean_version = clean_version[1:].strip()
    
    # æå–ç¬¬ä¸€ä¸ªç‰ˆæœ¬å·
    version_match = re.search(r'[vV]?(\d+\.\d+(?:\.\d+)?)', clean_version)
    if version_match:
        clean_version = version_match.group(0)
    
    if clean_version:
        return f"https://github.com/{owner}/{repo}/archive/refs/tags/{clean_version}.zip"
    return ""


def enrich_cve_data(cve_id: str, cve_data: Dict) -> Dict:
    """å¢å¼ºå•ä¸ª CVE çš„æ•°æ®"""
    print(f"  Processing {cve_id}...")
    
    # è·å–åŸå§‹ CVE æ•°æ®
    cve_raw = get_cve_raw_data(cve_id)
    if not cve_raw:
        print(f"    âš ï¸ No raw data found")
        return cve_data
    
    # æå–æ‰€æœ‰ URL
    urls = extract_all_urls(cve_raw)
    
    owner, repo = None, None
    
    # å¤„ç† patch commits
    if not cve_data.get("patch_commits") or all(not c.get("content") for c in cve_data.get("patch_commits", [])):
        new_commits = []
        for url in urls:
            info = extract_github_info_from_url(url)
            if info and info['type'] == 'commit':
                owner, repo = info['owner'], info['repo']
                print(f"    ğŸ“¥ Fetching commit {info['commit'][:8]}...")
                content = fetch_commit_content(owner, repo, info['commit'])
                new_commits.append({
                    "url": url,
                    "content": content
                })
                time.sleep(0.5)  # Rate limiting
        
        if new_commits:
            cve_data["patch_commits"] = new_commits
    
    # å¤„ç† security advisories
    if not cve_data.get("sec_adv") or all(not a.get("content") for a in cve_data.get("sec_adv", [])):
        new_advisories = []
        for url in urls:
            if "security/advisories" in url or "GHSA" in url:
                info = extract_github_info_from_url(url)
                if info and info['type'] == 'advisory':
                    owner, repo = info['owner'], info['repo']
                
                print(f"    ğŸ“¥ Fetching advisory...")
                content = fetch_advisory_content(url)
                
                # åˆ¤æ–­æ˜¯å¦æœ‰æ•ˆï¼ˆåŒ…å« PoC æˆ–è¯¦ç»†æ­¥éª¤ï¼‰
                content_lower = content.lower()
                has_poc = any(kw in content_lower for kw in ['poc', 'proof of concept', 'exploit', 'payload', 'curl', 'python', 'script'])
                
                new_advisories.append({
                    "url": url,
                    "content": content,
                    "effective": has_poc,
                    "effective_reason": "Contains PoC or exploit details" if has_poc else "No clear PoC found"
                })
                time.sleep(0.5)
        
        if new_advisories:
            cve_data["sec_adv"] = new_advisories
    
    # ç”Ÿæˆ sw_version_wget
    if not cve_data.get("sw_version_wget") and owner and repo:
        version = cve_data.get("sw_version", "")
        wget_url = generate_wget_url(owner, repo, version)
        if wget_url:
            cve_data["sw_version_wget"] = wget_url
    
    return cve_data


def main():
    print("=" * 60)
    print("CVE æ•°æ®å¢å¼ºè„šæœ¬")
    print("=" * 60)
    
    if not GITHUB_TOKEN:
        print("\nâš ï¸ è­¦å‘Š: æœªè®¾ç½® GITHUB_TOKEN ç¯å¢ƒå˜é‡")
        print("   API è¯·æ±‚å°†å—åˆ°ä¸¥æ ¼çš„é€Ÿç‡é™åˆ¶")
        print("   å»ºè®®è®¾ç½®: $env:GITHUB_TOKEN='your_token'")
    
    # åŠ è½½ç°æœ‰æ•°æ®
    print(f"\n[1/3] åŠ è½½ {WEBDATA_FILE}...")
    with open(WEBDATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"  - å…± {len(data)} ä¸ª CVE")
    
    # ç»Ÿè®¡ç¼ºå¤±å­—æ®µ
    missing_commits = sum(1 for v in data.values() if not v.get("patch_commits"))
    missing_advisory = sum(1 for v in data.values() if not v.get("sec_adv"))
    missing_wget = sum(1 for v in data.values() if not v.get("sw_version_wget"))
    
    print(f"\n  ç¼ºå¤±ç»Ÿè®¡:")
    print(f"  - patch_commits: {missing_commits} ä¸ªç¼ºå¤±")
    print(f"  - sec_adv: {missing_advisory} ä¸ªç¼ºå¤±")
    print(f"  - sw_version_wget: {missing_wget} ä¸ªç¼ºå¤±")
    
    # å¢å¼ºæ•°æ®
    print(f"\n[2/3] å¢å¼º CVE æ•°æ®...")
    enriched_count = 0
    
    for i, (cve_id, cve_data) in enumerate(data.items()):
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢å¼º
        needs_enrichment = (
            not cve_data.get("patch_commits") or 
            not cve_data.get("sec_adv") or
            not cve_data.get("sw_version_wget")
        )
        
        if needs_enrichment:
            data[cve_id] = enrich_cve_data(cve_id, cve_data)
            enriched_count += 1
        
        # è¿›åº¦æŠ¥å‘Š
        if (i + 1) % 10 == 0:
            print(f"\n  è¿›åº¦: {i + 1}/{len(data)} (å¢å¼ºäº† {enriched_count} ä¸ª)")
    
    # ä¿å­˜ç»“æœ
    print(f"\n[3/3] ä¿å­˜åˆ° {OUTPUT_FILE}...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    
    # æœ€ç»ˆç»Ÿè®¡
    final_missing_commits = sum(1 for v in data.values() if not v.get("patch_commits"))
    final_missing_advisory = sum(1 for v in data.values() if not v.get("sec_adv"))
    final_missing_wget = sum(1 for v in data.values() if not v.get("sw_version_wget"))
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"   å¢å¼ºäº† {enriched_count} ä¸ª CVE")
    print(f"\n  æœ€ç»ˆç¼ºå¤±ç»Ÿè®¡:")
    print(f"  - patch_commits: {final_missing_commits} ä¸ªç¼ºå¤± (åŸ {missing_commits})")
    print(f"  - sec_adv: {final_missing_advisory} ä¸ªç¼ºå¤± (åŸ {missing_advisory})")
    print(f"  - sw_version_wget: {final_missing_wget} ä¸ªç¼ºå¤± (åŸ {missing_wget})")


if __name__ == "__main__":
    main()
