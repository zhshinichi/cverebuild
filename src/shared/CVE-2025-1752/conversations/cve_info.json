{
    "published_date": "2025-05-10T13:21:30.866Z",
    "patch_commits": [
        {
            "url": "https://github.com/run-llama/llama_index/commit/3c65db2947271de3bd1927dc66a044da385de4da",
            "content": "fix: respect max_depth in KnowledgeBaseWebReader (#17949)\n\nFilename: llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/knowledge_base/base.py:\n```\n@@ -127,7 +127,12 @@\ndef scrape_article(\n         return {\"title\": title, \"subtitle\": subtitle, \"body\": body, \"url\": url}\n \n     def get_article_urls(\n-        self, browser: Any, root_url: str, current_url: str, max_depth: int = 100\n+        self,\n+        browser: Any,\n+        root_url: str,\n+        current_url: str,\n+        max_depth: int = 100,\n+        depth: int = 0,\n     ) -> List[str]:\n         \"\"\"\n         Recursively crawl through the knowledge base to find a list of articles.\n\n@@ -136,11 +141,17 @@\ndef get_article_urls(\n             browser (Any): a Playwright Chromium browser.\n             root_url (str): root URL of the knowledge base.\n             current_url (str): current URL that is being crawled.\n+            max_depth (int): maximum recursion level for the crawler\n+            depth (int): current depth level\n \n         Returns:\n             List[str]: a list of URLs of found articles.\n \n         \"\"\"\n+        if depth >= max_depth:\n+            print(f\"Reached max depth ({max_depth}): {current_url}\")\n+            return []\n+\n         page = browser.new_page(ignore_https_errors=True)\n         page.set_default_timeout(60000)\n         page.goto(current_url, wait_until=\"domcontentloaded\")\n\n@@ -162,7 +173,7 @@\ndef get_article_urls(\n         for link in links:\n             url = root_url + page.evaluate(\"(node) => node.getAttribute('href')\", link)\n             article_urls.extend(\n-                self.get_article_urls(browser, root_url, url, max_depth)\n+                self.get_article_urls(browser, root_url, url, max_depth, depth + 1)\n             )\n \n         page.close()\n```\n\nFilename: llama-index-integrations/readers/llama-index-readers-web/pyproject.toml:\n```\n@@ -46,7 +46,7 @@\nlicense = \"GPL-3.0-or-later\"\n maintainers = [\"HawkClaws\", \"Hironsan\", \"NA\", \"an-bluecat\", \"bborn\", \"jasonwcfan\", \"kravetsmic\", \"pandazki\", \"ruze00\", \"selamanse\", \"thejessezhang\"]\n name = \"llama-index-readers-web\"\n readme = \"README.md\"\n-version = \"0.3.5\"\n+version = \"0.3.6\"\n \n [tool.poetry.dependencies]\n python = \">=3.9,<4.0\"\n```"
        }
    ],
    "sw_version": "0.3.5",
    "sw_version_wget": "https://github.com/run-llama/llama_index/archive/refs/tags/v0.3.5.zip",
    "description": "[Affected Package: llama-index-readers-web==0.3.5] A Denial of Service (DoS) vulnerability has been identified in the KnowledgeBaseWebReader class of the run-llama/llama_index project, affecting version ~ latest(v0.12.15). The vulnerability arises due to inappropriate secure coding measures, specifically the lack of proper implementation of the max_depth parameter in the get_article_urls function. This allows an attacker to exhaust Python's recursion limit through repeated function calls, leading to resource consumption and ultimately crashing the Python process.",
    "sec_adv": [
        {
            "url": "https://huntr.com/bounties/cd7b9082-7d75-42e4-84f5-dbee23cbc467",
            "content": "Bounties\nPartners\nCommunity\nInfo\nSUBMIT REPORT\nA DoS attack occurred in run-llama/llama_index due to inappropriate secure coding measures in run-llama/llama_index\nValid\nReported on Feb 2nd 2025\nDescription\nA DoS vulnerability has been identified in the KnowledgeBaseWebReader class of the run-llama/llama_index project, and this issue has been reported (see the link below):\nHuntr Report : https://huntr.com/bounties/27883f22-35ff-49df-aaa5-05031c7d6ad8\nHowever, due to the developer's inappropriate mitigation, the DoS vulnerability still persists. As a result, an attacker can perform a DoS attack using the same payload as reported in the link above.\nThe source code of the KnowledgeBaseWebReader class containing the issue\n    def get_article_urls(\n        self, browser: Any, root_url: str, current_url: str, max_depth: int = 100\n    ) -> List[str]:\n        \"\"\"\n        Recursively crawl through the knowledge base to find a list of articles.\n\n        Args:\n            browser (Any): a Playwright Chromium browser.\n            root_url (str): root URL of the knowledge base.\n            current_url (str): current URL that is being crawled.\n\n        Returns:\n            List[str]: a list of URLs of found articles.\n\n        \"\"\"\n        page = browser.new_page(ignore_https_errors=True)\n        page.set_default_timeout(60000)\n        page.goto(current_url, wait_until=\"domcontentloaded\")\n\n        # If this is a leaf node aka article page, return itself\n        if self.article_path in current_url:\n            print(\"Found an article: \", current_url)\n            page.close()\n            return [current_url]\n\n        # Otherwise crawl this page and find all the articles linked from it\n        article_urls = []\n        links = []\n\n        for link_selector in self.link_selectors:\n            ahrefs = page.query_selector_all(link_selector)\n            links.extend(ahrefs)\n\n        for link in links:\n            url = root_url + page.evaluate(\"(node) => node.getAttribute('href')\", link)\n            article_urls.extend(\n                self.get_article_urls(browser, root_url, url, max_depth)\n            )\n\n        page.close()\n\n        return article_urls\nPlease refer to the previously reported Huntr Report link for details on why this code leads to a DoS vulnerability.\nThe developer attempted to prevent the DoS vulnerability by adding max_depth: int = 100, but the secure coding measure was not properly implemented.\nAs the loop progresses, the max_depth value should increment, and execution should terminate when the threshold is reached. However, the current code lacks logic for increasing the threshold or executing the termination process.\ndef get_article_urls(\n    self, browser: Any, root_url: str, current_url: str, max_depth: int, depth: int = 0\n) -> List[str]:\n\n    # Execution terminates when max_depth is reached.\n    if depth >= max_depth:\n        print(f\"Reached max depth ({max_depth}): {current_url}\")\n        return []\n\n    for link in links:\n        href = page.evaluate(\"(node) => node.getAttribute('href')\", link)\n        if href:\n            url = root_url + href if href.startswith(\"/\") else href\n\n            # The depth increases by +1 on each recursive call.\n            article_urls.extend(\n                self.get_article_urls(browser, root_url, url, max_depth, depth + 1)\n            )\n\n    page.close()\n    return article_urls\nPatch Recommendation\nIt is recommended to modify the value of max_depth as the loop progresses, as shown above. When the threshold is reached, the program should either terminate or raise an exception.\nImpact\nThis vulnerability can exhaust Python's recursion limit through repeated function calls, leading to resource consumption and ultimately crashing the Python process.\nOccurrences\nbase.py L129-L170\nWe are processing your report and will contact therun-llama/llama_index team within 24 hours.3 months ago\nA GitHub Issue asking the maintainers to create a SECURITY.md exists3 months ago\nhuntr-helperhas marked this vulnerability as a duplicate of 27883f22-35ff-49df-aaa5-05031c7d6ad83 months ago\nThe disclosure bounty has been dropped\nThe fix bounty has been dropped\nThe researcher's credibility has decreased: -5\nhuntr-helper\ncommented3 months ago\nAdmin\nThis report was closed because it was determined to be a duplicate of a report submitted on 2024-10-28 04:42:27.900000\nA huntr admin reset this report to a pending state.3 months ago\nSirius\ncommented3 months ago\nResearcher\nHello, is the review of the above content in progress?\nMassimiliano Pippi validated this vulnerability3 months ago\nThe researcher is right, fix wasn't effective.\nsiriusbellatrixhas been awarded the disclosure bounty\nThe fix bounty is now up for grabs\nThe researcher's credibility has increased: +7\nCVE-2025-1752assigned to this report.3 months ago\nMassimiliano Pippi\ncommented3 months ago\nMaintainer\nhttps://github.com/run-llama/llama_index/pull/17949\nA run-llama/llama_index maintainerhas acknowledged this report3 months ago\nThe scheduled publication date was automatically extended from 3rd May 2025  to 10th May 2025  due to the maintainers acknowledgement of the report3 months ago\nMassimiliano Pippimarked this as fixedin 0.3.6with commit3c65db3 months ago\nMassimiliano Pippihas been awarded the fix bounty\nbase.py#L129-L170 has been validated\nWe have notified the run-llama/llama_index maintainers about this report in their weekly follow-up11 days ago\nWe have sent a warning to the run-llama/llama_index team to inform them that this report will be published in 48 hours8 days ago\nA  run-llama/llama_index maintainer\ncommented8 days ago\nMaintainer\nFyi, the title of this issue is not accurate. This affects a specific package in our monorepo, not the entire repo or main package\nllama-index-readers-web in llama-index-integrations/readers/llama-index-readers-web\nThis vulnerability has now been published6 days ago\nCVE-2025-1752has now been published6 days ago\nSign in to join this conversation\nCVE\nCVE-2025-1752\n(Published, Under Review)\nVulnerability Type\nCWE-400: Denial of Service\nSeverity\nHigh (7.5)\nAttack vector\nNetwork\nAttack complexity\nLow\nPrivileges required\nNone\nUser interaction\nNone\nScope\nUnchanged\nConfidentiality\nNone\nIntegrity\nNone\nAvailability\nHigh\nOpen in visual CVSS calculator\nRegistry\nPypi\nAffected Version\n~ latest(v0.12.15)\nVisibility\nPublic\nStatus\nFixed\nDisclosure Bounty\n$750\nFix Bounty\n$187.5\nFound by\nFixed by\nSupported by Protect AI and leading the way to MLSecOps and greater AI security.\n\u00a9 2024\nPrivacy Policy\nTerms of Service\nCode of Conduct\nCookie Preferences\nContact Us",
            "effective": false,
            "effective_reason": "The advisory provides details about the vulnerability but does not include an explicit proof of concept or reproduction steps."
        }
    ],
    "cwe": [
        {
            "id": "CWE-400",
            "value": "CWE-400 Uncontrolled Resource Consumption"
        }
    ],
    "pip_package": "llama-index-readers-web",
    "pip_version": "0.3.5",
    "sw_version_wget_note": "NOTE: This wget URL points to the main llama_index repo v0.3.5 (Feb 2023), which does NOT contain the vulnerable KnowledgeBaseWebReader class. The vulnerability exists in the separate pip package 'llama-index-readers-web==0.3.5'. Use 'pip install llama-index-readers-web==0.3.5' instead of downloading from GitHub.",
    "dir_tree": ".\n\u2514\u2500\u2500 llama_index\n    \u2514\u2500\u2500 readers\n        \u2514\u2500\u2500 web\n            \u251c\u2500\u2500 agentql_web\n            \u251c\u2500\u2500 async_web\n            \u251c\u2500\u2500 beautiful_soup_web\n            \u251c\u2500\u2500 browserbase_web\n            \u251c\u2500\u2500 firecrawl_web\n            \u251c\u2500\u2500 hyperbrowser_web\n            \u251c\u2500\u2500 knowledge_base\n            \u251c\u2500\u2500 main_content_extractor\n            \u251c\u2500\u2500 news\n            \u251c\u2500\u2500 readability_web\n            \u251c\u2500\u2500 rss\n            \u251c\u2500\u2500 rss_news\n            \u251c\u2500\u2500 scrapfly_web\n            \u251c\u2500\u2500 simple_web\n            \u251c\u2500\u2500 sitemap\n            \u251c\u2500\u2500 spider_web\n            \u251c\u2500\u2500 trafilatura_web\n            \u251c\u2500\u2500 unstructured_web\n            \u251c\u2500\u2500 whole_site\n            \u2514\u2500\u2500 zyte_web\n\n23 directories\n",
    "repo_path": "llama_index_readers_web-0.3.5/"
}